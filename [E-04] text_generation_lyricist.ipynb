{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2faaf3a1",
   "metadata": {},
   "source": [
    "#  <span style=\"\"> 인공지능 작사가 만들기 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89caae5",
   "metadata": {},
   "source": [
    "# <span style=\"\"> 1.데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df73efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof', 'You saw her bathing on the roof', 'Her beauty and the moonlight overthrew her', 'She tied you', 'To a kitchen chair', 'She broke your throne, and she cut your hair', 'And from your lips she drew the Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah You say I took the name in vain', \"I don't even know the name\", \"But if I did, well really, what's it to you?\", \"There's a blaze of light\", 'In every word', \"It doesn't matter which you heard\", 'The holy or the broken Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', \"Hallelujah I did my best, it wasn't much\", \"I couldn't feel, so I tried to touch\", \"I've told the truth, I didn't come to fool you\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 리스트에 문장 단위로 저장 \n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 읽어오기\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0cf29",
   "metadata": {},
   "source": [
    "# <span style=\"\">2.데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a6716e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n",
      "You saw her bathing on the roof\n",
      "Her beauty and the moonlight overthrew her\n",
      "She tied you\n",
      "To a kitchen chair\n",
      "She broke your throne, and she cut your hair\n",
      "And from your lips she drew the Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah You say I took the name in vain\n",
      "I don't even know the name\n",
      "But if I did, well really, what's it to you?\n",
      "There's a blaze of light\n",
      "In every word\n",
      "It doesn't matter which you heard\n",
      "The holy or the broken Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah I did my best, it wasn't much\n",
      "I couldn't feel, so I tried to touch\n",
      "I've told the truth, I didn't come to fool you\n",
      "And even though\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    \n",
    "    raw_corpus.append(sentence)\n",
    "    \n",
    "    if idx > 30: break   # 문장 30개만 확인\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ddd36",
   "metadata": {},
   "source": [
    "# <span style=\"\">3. 토큰화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b02cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)       # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)              # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여줌\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5292b6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>',\n",
       " '<start> you saw her bathing on the roof <end>',\n",
       " '<start> her beauty and the moonlight overthrew her <end>',\n",
       " '<start> she tied you <end>',\n",
       " '<start> to a kitchen chair <end>',\n",
       " '<start> she broke your throne , and she cut your hair <end>',\n",
       " '<start> and from your lips she drew the hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah you say i took the name in vain <end>',\n",
       " '<start> i don t even know the name <end>',\n",
       " '<start> but if i did , well really , what s it to you ? <end>',\n",
       " '<start> there s a blaze of light <end>',\n",
       " '<start> in every word <end>',\n",
       " '<start> it doesn t matter which you heard <end>',\n",
       " '<start> the holy or the broken hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah i did my best , it wasn t much <end>',\n",
       " '<start> i couldn t feel , so i tried to touch <end>',\n",
       " '<start> i ve told the truth , i didn t come to fool you <end>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "     \n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "      \n",
    "\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2f5d2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152462\n",
      "[[  2  50   5 ...   0   0   0]\n",
      " [  2   5  22 ...   0   0   0]\n",
      " [  2  39  39 ...   0   0   0]\n",
      " ...\n",
      " [  2   5  91 ... 557   7   3]\n",
      " [  2   8 163 ...   0   0   0]\n",
      " [  2  11  24 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f5e15a96e80>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있음. 이번에는 사용하지 안함\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 <unk> 처리\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 구축한 corpus로부터 Tokenizer가 사전을 자동구축\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환.\n",
    "   \n",
    "    for num in tensor:\n",
    "        if len(num) >= 29:\n",
    "            tensor = np.delete(tensor, num)\n",
    "            \n",
    "    print(len(tensor))\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "    \n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24939607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 어떻게 생겼는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c096c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  296   64   57    9  954 5344    3    0    0    0]\n",
      "[  50    5   91  296   64   57    9  954 5344    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502778bf",
   "metadata": {},
   "source": [
    "# <span style=\"\">4. 평가 데이터셋 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e176b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (30492, 14)\n",
      "Target Train: (30492, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          train_size = 0.2,\n",
    "                                                          random_state=34)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "747844b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)         \n",
    "BATCH_SIZE = 256                     \n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 15000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f71fe",
   "metadata": {},
   "source": [
    "# <span style=\"\">5. 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efd1e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.drop  = tf.keras.layers.Dropout(0.5)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b56c1b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 15001), dtype=float32, numpy=\n",
       "array([[[-1.13382586e-04,  1.70813964e-05, -7.91161001e-05, ...,\n",
       "          2.53542821e-04,  2.41499380e-04,  4.52072818e-05],\n",
       "        [-4.87955440e-05,  7.89912083e-05, -1.82508258e-04, ...,\n",
       "          3.17413505e-04,  1.50673295e-04,  1.12281290e-04],\n",
       "        [ 1.66588186e-04,  9.88773027e-05, -2.75021972e-04, ...,\n",
       "          4.17029602e-04,  3.56435812e-05,  2.53980455e-04],\n",
       "        ...,\n",
       "        [-1.70830521e-03,  5.13673840e-05, -3.04716901e-04, ...,\n",
       "         -1.74028348e-04,  3.65449174e-04,  2.01012590e-03],\n",
       "        [-1.97751517e-03, -2.83453031e-04, -5.43377246e-04, ...,\n",
       "         -9.32563580e-06,  2.31377213e-04,  1.85591937e-03],\n",
       "        [-2.25726119e-03, -6.04155648e-04, -8.56899191e-04, ...,\n",
       "          2.56539788e-04,  8.57840569e-05,  1.64354092e-03]],\n",
       "\n",
       "       [[-1.13382586e-04,  1.70813964e-05, -7.91161001e-05, ...,\n",
       "          2.53542821e-04,  2.41499380e-04,  4.52072818e-05],\n",
       "        [-4.78985778e-04, -4.96693137e-05, -1.61506003e-04, ...,\n",
       "          5.11830789e-04,  3.74336058e-04, -4.39037867e-05],\n",
       "        [-6.15379831e-04, -1.83855343e-04, -1.62371158e-04, ...,\n",
       "          6.24898414e-04,  4.45143989e-04,  2.23118768e-04],\n",
       "        ...,\n",
       "        [-7.33697962e-04, -4.13773640e-04, -3.47241788e-04, ...,\n",
       "         -9.60738806e-04,  2.11838960e-05,  6.01327338e-04],\n",
       "        [-1.11735798e-03, -6.13395590e-04, -6.27500296e-04, ...,\n",
       "         -8.99116159e-04, -4.52750537e-05,  7.20367651e-04],\n",
       "        [-1.51069486e-03, -8.29385477e-04, -9.76164069e-04, ...,\n",
       "         -6.82673301e-04, -1.30627857e-04,  7.36573129e-04]],\n",
       "\n",
       "       [[-1.13382586e-04,  1.70813964e-05, -7.91161001e-05, ...,\n",
       "          2.53542821e-04,  2.41499380e-04,  4.52072818e-05],\n",
       "        [-6.52124945e-05, -1.90717867e-04, -1.22966434e-04, ...,\n",
       "          1.18805416e-04,  3.45121138e-04,  3.47279201e-05],\n",
       "        [-6.71938687e-05, -2.05141987e-04, -2.05742617e-04, ...,\n",
       "         -4.35539368e-05,  5.94281359e-04,  1.85354249e-04],\n",
       "        ...,\n",
       "        [-8.07881181e-04, -3.53192045e-05,  2.01981398e-04, ...,\n",
       "         -6.92969537e-04,  5.00426919e-04,  1.08459545e-03],\n",
       "        [-1.12528657e-03, -3.55266820e-04, -8.82730164e-05, ...,\n",
       "         -5.76727674e-04,  3.74107098e-04,  1.08928932e-03],\n",
       "        [-1.47720391e-03, -6.69869361e-04, -4.67189122e-04, ...,\n",
       "         -3.17629048e-04,  2.43088769e-04,  1.01172214e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.13382586e-04,  1.70813964e-05, -7.91161001e-05, ...,\n",
       "          2.53542821e-04,  2.41499380e-04,  4.52072818e-05],\n",
       "        [ 1.43348789e-05, -2.17486784e-04, -8.35770843e-05, ...,\n",
       "          3.81435413e-04,  5.25075418e-04,  1.85351731e-04],\n",
       "        [ 6.27530462e-05, -3.22303327e-04, -1.44427377e-05, ...,\n",
       "          4.58331604e-04,  7.04810489e-04,  9.42470160e-06],\n",
       "        ...,\n",
       "        [-3.53715412e-04, -1.70037081e-03, -2.86805989e-05, ...,\n",
       "         -6.39624428e-04, -1.13047012e-04, -8.08138284e-05],\n",
       "        [-5.00229129e-04, -1.67902349e-03, -1.87130398e-04, ...,\n",
       "         -7.71852385e-04, -1.36794042e-04,  2.11954612e-05],\n",
       "        [-7.40980729e-04, -1.66440301e-03, -4.67234699e-04, ...,\n",
       "         -7.30763422e-04, -1.60634969e-04,  1.12983260e-04]],\n",
       "\n",
       "       [[-1.13382586e-04,  1.70813964e-05, -7.91161001e-05, ...,\n",
       "          2.53542821e-04,  2.41499380e-04,  4.52072818e-05],\n",
       "        [-2.21236871e-04,  2.66321440e-04, -3.12269171e-04, ...,\n",
       "          5.53566089e-04,  1.69897554e-04,  1.30278800e-04],\n",
       "        [-4.99706715e-04,  4.55729198e-04, -6.69191242e-04, ...,\n",
       "          8.18561180e-04,  2.34062420e-04,  3.60764068e-04],\n",
       "        ...,\n",
       "        [-2.39217980e-03, -8.53318430e-04, -1.66880083e-03, ...,\n",
       "          6.58153149e-04, -2.35991261e-04,  9.56085918e-04],\n",
       "        [-2.71217781e-03, -1.14244292e-03, -2.03289860e-03, ...,\n",
       "          1.05017226e-03, -3.34479002e-04,  7.76339322e-04],\n",
       "        [-3.00650625e-03, -1.38414640e-03, -2.38214526e-03, ...,\n",
       "          1.44854083e-03, -4.26435494e-04,  5.91355318e-04]],\n",
       "\n",
       "       [[-1.13382586e-04,  1.70813964e-05, -7.91161001e-05, ...,\n",
       "          2.53542821e-04,  2.41499380e-04,  4.52072818e-05],\n",
       "        [-2.14355590e-04, -4.37980198e-05, -2.98506668e-04, ...,\n",
       "          3.31393676e-04,  3.65419954e-04,  4.50765219e-05],\n",
       "        [-4.01186466e-04,  8.89586008e-05, -6.76177850e-04, ...,\n",
       "          4.32962814e-04,  4.58265888e-04,  1.79882380e-04],\n",
       "        ...,\n",
       "        [-2.70778104e-03, -1.15911756e-03, -2.36696075e-03, ...,\n",
       "          1.56986481e-03, -4.88771650e-04, -1.64397115e-05],\n",
       "        [-2.96197296e-03, -1.35128212e-03, -2.70410161e-03, ...,\n",
       "          1.91342528e-03, -5.69598167e-04, -1.17319425e-04],\n",
       "        [-3.19817173e-03, -1.51228730e-03, -3.00973351e-03, ...,\n",
       "          2.24006199e-03, -6.40795217e-04, -2.13380947e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3081acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  30737049  \n",
      "=================================================================\n",
      "Total params: 87,022,489\n",
      "Trainable params: 87,022,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c772b4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "595/595 [==============================] - 284s 474ms/step - loss: 3.7057\n",
      "Epoch 2/10\n",
      "595/595 [==============================] - 284s 477ms/step - loss: 3.1679\n",
      "Epoch 3/10\n",
      "595/595 [==============================] - 284s 477ms/step - loss: 2.9534\n",
      "Epoch 4/10\n",
      "595/595 [==============================] - 284s 476ms/step - loss: 2.7727\n",
      "Epoch 5/10\n",
      "595/595 [==============================] - 284s 477ms/step - loss: 2.6060\n",
      "Epoch 6/10\n",
      "595/595 [==============================] - 285s 478ms/step - loss: 2.4484\n",
      "Epoch 7/10\n",
      "595/595 [==============================] - 284s 477ms/step - loss: 2.2995\n",
      "Epoch 8/10\n",
      "595/595 [==============================] - 284s 477ms/step - loss: 2.1580\n",
      "Epoch 9/10\n",
      "595/595 [==============================] - 285s 478ms/step - loss: 2.0247\n",
      "Epoch 10/10\n",
      "595/595 [==============================] - 285s 478ms/step - loss: 1.8988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c54053d90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a35e08",
   "metadata": {},
   "source": [
    "# <span style=\"\">6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80e6d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f07c072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m gonna be a little selfish <end> '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e98df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am the one who s gonna be <end> '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b529d915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39d59320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> we re gonna make it babe , we re gonna make it babe <end> '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> we\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e561861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> study me , hit me , break me <end> '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7507b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i have a little bit of my heart <end> '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i have\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181e8eb",
   "metadata": {},
   "source": [
    "# <span style=\"\">회고\n",
    "\n",
    "작사가 AI만들기를 진행했다.\n",
    "막상 시간이 많이 걸리고 커널 연결도 계속 끝어져서 중단되어 다시 만드는 일이 반복되어\n",
    "많은 시간이 소모되었다. 또한 내용도 완전히 이해되지 못한 채 진행이 되어 붙여넣기 수준으로 코드가\n",
    "작성되어 많은 자괴감이 들었다. 앞으로 많은 공부가 필요할 듯 하다.\n",
    "\n",
    "hidden_size를 1024에서 2048로 바꾸어 진행하고 epochs값을 20에서 10으로 하여 학습 시간을 줄였다.\n",
    "단어는 15000개로 설정하고 데이터의 20%를 평가 데이터셋으로 사용하였다.\n",
    "텍스트 생성모델의 validation loss 또한 1.8로 안정적으로 학습되었고,\n",
    "가사 택스트 생성 모델이 정상적으로 작동하여\n",
    "\n",
    "<start> i m gonna be a little selfish <end> \n",
    "    \n",
    "<start> i am the one who s gonna be <end>\n",
    "    \n",
    "<start> i love you <end>\n",
    "    \n",
    "<start> i have a little bit of my heart <end>\n",
    "\n",
    "위와 같이, 문장 또한 잘 만들어 졌다. 작사가 자동으로 된다는 신기하고 재미있는 경험이었다.\n",
    "하지만 만들어진 문장이 간단하고 세련되지 못한 느낌을 받았다.\n",
    "\n",
    "많이 부족한 공부였지만 프로젝트를 통해 인공지능이 문장을 이해하는 방식에 대해 좀 더 이해할 수 있게 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956f414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
